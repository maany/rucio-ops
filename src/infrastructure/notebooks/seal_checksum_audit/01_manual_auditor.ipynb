{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Projects/rucio-ops/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Projects/rucio-ops/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/Projects/rucio-ops/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.plotting.backend='matplotlib'\n",
    "# set cwd to 3 directory up\n",
    "%pwd\n",
    "%cd ../../../\n",
    "%pwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAL dumps: Serialize into TimeRangeSourceData\n",
    "---\n",
    "\n",
    "In this step, we load the files that contain the dumps provided by SEAL and store them in a TimeRangeSourceData format. This format is a dictionary that contains the following keys:\n",
    "start: the start time of the dump\n",
    "end: the end time of the dump\n",
    "file: the file that contains the dump\n",
    "source: the source of the dump ( SEAL or Rucio ). In this case, it will be SEAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_648204/1826891564.py:12: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"{dir}/{file}.csv\")\n"
     ]
    }
   ],
   "source": [
    "from infrastructure.repository.data_repository import list_files\n",
    "from datetime import datetime\n",
    "from core.entity import TimeRangeSourceData\n",
    "dir = 'data/seal'\n",
    "data_files = list_files(dir)\n",
    "data_files = [x.split('.')[0] for x in data_files]\n",
    "seal_dumps_time_ranges = []\n",
    "for file in data_files:\n",
    "    _, start_date, end_date = file.split('_')\n",
    "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "    df = pd.read_csv(f\"{dir}/{file}.csv\")\n",
    "    seal_dumps_time_ranges.append(TimeRangeSourceData(start=start_date, end=end_date, source='SEAL', file=f\"{dir}/{file}.csv\", df=df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the continuity of the time ranges\n",
    "\n",
    "For all the files that we have, we will check if the time intervals are chained. This means that the end time of a dump is the same as the start time of the next dump. If this is not the case, we will print an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by start date\n",
    "seal_dumps_time_ranges = sorted(seal_dumps_time_ranges, key=lambda x: x.start)\n",
    "\n",
    "# check if the entrires form a continuous time range\n",
    "for i in range(1, len(seal_dumps_time_ranges)):\n",
    "    if seal_dumps_time_ranges[i].start != seal_dumps_time_ranges[i-1].end:\n",
    "        print(f\"Continuity Error: {seal_dumps_time_ranges[i-1].end} != {seal_dumps_time_ranges[i].start}\")\n",
    "        print(f\"Check files {seal_dumps_time_ranges[i-1].file} and {seal_dumps_time_ranges[i].file}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the data across different time ranges\n",
    "\n",
    "### Shortlist the final set of time ranges that form a continuous chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_time_ranges = seal_dumps_time_ranges[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the time ranges to form a single time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seal_dumps_concatenated_df = pd.concat([x.df for x in selected_time_ranges])\n",
    "\n",
    "# sort by mtime, then by name\n",
    "seal_dumps_concatenated_df = seal_dumps_concatenated_df.sort_values(by=['mtime', 'name'])\n",
    "\n",
    "# convert mtime to datetime\n",
    "seal_dumps_concatenated_df['mtime'] = pd.to_datetime(seal_dumps_concatenated_df['mtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a month and year key to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a month column\n",
    "seal_dumps_concatenated_df['month'] = seal_dumps_concatenated_df['mtime'].dt.month\n",
    "\n",
    "# add a year column\n",
    "seal_dumps_concatenated_df['year'] = seal_dumps_concatenated_df['mtime'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seal_dumps_concatenated_df[(seal_dumps_concatenated_df['month'] == 5) & (seal_dumps_concatenated_df['year'] == 2022) ])\n",
    "# seal_dumps_concatenated_df[\"year\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rucio Dumps: Serialize Rucio dumps into Pandas DataFrame\n",
    "---\n",
    "\n",
    "In this step, we load the files that contain the dumps provided by Rucio and store them in a Pandas Dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rucio_dumps_file = 'data/rucio/SEAL_TEST_2023-04-10'\n",
    "data = []\n",
    "with open(rucio_dumps_file) as f:\n",
    "    rucio_dumps_data = f.readlines()\n",
    "    for rucio_dumps_row in rucio_dumps_data:\n",
    "        columns = rucio_dumps_row.split('\\t')\n",
    "        columns = [x.strip() for x in columns]\n",
    "        rse, scope, name, checksum, size, creation_date, path, update_date, state , _, _, _= columns\n",
    "        __file_name = path.split('/')[-1]\n",
    "        __start_path = '/'.join(path.split('/')[0:-3])\n",
    "        path = f\"{__start_path}/{__file_name}\"\n",
    "        if(path.startswith('/')):\n",
    "            path = path[1:]\n",
    "        \n",
    "        size = int(size)\n",
    "        creation_date = datetime.strptime(creation_date, '%Y-%m-%d %H:%M:%S')\n",
    "        update_date = datetime.strptime(update_date, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        data.append([rse, scope, name, checksum, size, creation_date, path, update_date, state])\n",
    "\n",
    "rucio_dumps_df = pd.DataFrame(columns=['rse', 'scope', 'name', 'checksum', 'size', 'creation_date', 'path', 'update_date', 'state'], data=data)\n",
    "\n",
    "# sort by creation date\n",
    "rucio_dumps_df = rucio_dumps_df.sort_values(by=['creation_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a month and year key to each row of rucio dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rucio_dumps_df['month'] = rucio_dumps_df['creation_date'].dt.month\n",
    "rucio_dumps_df['year'] = rucio_dumps_df['creation_date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost DIDs\n",
    "\n",
    "We will check if there are any files that are present in the Rucio dumps but not in the SEAL dumps. If there are any such files, these files are marked as lost files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rucio_dumps_df.set_index('path')\n",
    "seal_dumps_concatenated_df.set_index('path')\n",
    "\n",
    "lost_dids = rucio_dumps_df[~rucio_dumps_df['path'].isin(seal_dumps_concatenated_df['path'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dark DIDs\n",
    "\n",
    "We will check if there are any files that are present in the SEAL dumps but not in the Rucio dumps. If there are any such files, these files are marked as dark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_dids = seal_dumps_concatenated_df[~seal_dumps_concatenated_df['path'].isin(rucio_dumps_df['path'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range_start = seal_dumps_time_ranges[0].start\n",
    "time_range_end = seal_dumps_time_ranges[-1].end\n",
    "print(f\"This report is generated from {time_range_start} to {time_range_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost DIDs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rucio_dids = rucio_dumps_df['name'].nunique()\n",
    "percentage_lost_num_dids = round((lost_dids.shape[0] / num_rucio_dids) * 100, 2)\n",
    "\n",
    "print(f\"Number of Rucio DIDs: {num_rucio_dids}\")\n",
    "print(f\"Number of lost files: {lost_dids.shape[0]}\")\n",
    "print(f\"Percentage of Lost Files: {percentage_lost_num_dids}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invalidate the files reported as lost that do not fall in the time period under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_lost_files = lost_dids[(lost_dids['creation_date'] < time_range_start) | (lost_dids['creation_date'] > time_range_end)]\n",
    "\n",
    "print(f\"Number of invalid lost files: {invalid_lost_files.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import bytesToTB\n",
    "total_size_rucio = rucio_dumps_df['size'].sum()\n",
    "size_lost_files = lost_dids['size'].sum()\n",
    "percentage_lost_size = round((size_lost_files / total_size_rucio) * 100, 2)\n",
    "\n",
    "\n",
    "print(f\"Total size of Rucio DIDs: {bytesToTB(total_size_rucio)} TB\")\n",
    "print(f\"Total size of Lost DIDs: {bytesToTB(size_lost_files)} TB\")\n",
    "print(f\"Percentage of Lost Size: {percentage_lost_size}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Available', 'Lost']\n",
    "sizes = [total_size_rucio - size_lost_files, size_lost_files]\n",
    "\n",
    "colors = ['olivedrab','saddlebrown']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].pie([num_rucio_dids - lost_dids.shape[0], lost_dids.shape[0]], labels=['Available', 'Lost'], autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axs[0].set_title('Lost Data ( by Number of DIDs )')\n",
    "axs[0].set_xlabel(f'Total DIDs: {num_rucio_dids}')\n",
    "\n",
    "axs[1].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axs[1].set_title('Lost Data ( by Size )')\n",
    "axs[1].set_xlabel(f'Total Size: {bytesToTB(total_size_rucio)} TB')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dark DIDs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seal_dids = seal_dumps_concatenated_df['name'].nunique()\n",
    "num_dark_dids = dark_dids['name'].nunique()\n",
    "percentage_dark = round((dark_dids.shape[0] / num_seal_dids) * 100, 2)\n",
    "\n",
    "print(f\"Number of SEAL DIDs: {num_seal_dids}\")\n",
    "print(f\"Number of Dark Files: {dark_dids.shape[0]}\")\n",
    "print(f\"Percentage of Dark Data: {percentage_dark}%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_seal = seal_dumps_concatenated_df['size'].sum()\n",
    "size_dark_files = dark_dids['size'].sum()\n",
    "percentage_dark_size = round((size_dark_files / total_size_seal) * 100, 2)\n",
    "\n",
    "print(f\"Total size of SEAL DIDs: {bytesToTB(total_size_seal)} TB\")\n",
    "print(f\"Total size of Dark Data: {bytesToTB(size_dark_files)} TB\")\n",
    "print(f\"Percentage of Dark Size: {percentage_dark_size}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "sizes = [total_size_seal - size_dark_files, size_dark_files]\n",
    "\n",
    "axs[0].pie([num_seal_dids - dark_dids.shape[0], dark_dids.shape[0]], labels=['Available', 'Dark'], autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axs[0].set_title('Dark Data ( by Number of DIDs )')\n",
    "axs[0].set_xlabel(f'Total DIDs: {num_seal_dids}')\n",
    "\n",
    "axs[1].pie(sizes, labels=[\"Available\", \"Dark\"], autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axs[1].set_title('Dark Data ( by Size )')\n",
    "axs[1].set_xlabel(f'Total Size: {bytesToTB(total_size_seal)} TB')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent DIDs\n",
    "\n",
    "Consistent DIDs are present in both Rucio and SEAL dumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistent_dids = rucio_dumps_df[rucio_dumps_df['path'].isin(seal_dumps_concatenated_df['path'])]\n",
    "\n",
    "num_consistent_dids = consistent_dids['name'].nunique()\n",
    "percentage_consistent = round((num_consistent_dids / num_rucio_dids) * 100, 2)\n",
    "\n",
    "print(f\"Number of Consistent DIDs: {num_consistent_dids}\")\n",
    "print(f\"Percentage of Consistent DIDs: {percentage_consistent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_consistent_files = consistent_dids['size'].sum()\n",
    "percentage_consistent_size = round((size_consistent_files / total_size_rucio) * 100, 2)\n",
    "\n",
    "print(f\"Total size of Consistent DIDs: {bytesToTB(size_consistent_files)} TB\")\n",
    "print(f\"Percentage of Consistent Size: {percentage_consistent_size}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = time_range_start.strftime('%Y%m%d')\n",
    "end_date = time_range_end.strftime('%Y%m%d')\n",
    "lost_dids.to_csv(f'data/outputs/{start_date}-{end_date}_lost_dids.csv', index=False)\n",
    "dark_dids.to_csv(f'data/outputs/{start_date}-{end_date}_dark_dids.csv', index=False)\n",
    "consistent_dids.to_csv(f'data/outputs/{start_date}-{end_date}_consistent_dids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_range_start, time_range_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into month, year tuples\n",
    "lost_dids['month_year'] = list(zip(lost_dids['month'], lost_dids['year']))\n",
    "dark_dids['month_year'] = list(zip(dark_dids['month'], dark_dids['year']))\n",
    "consistent_dids['month_year'] = list(zip(consistent_dids['month'], consistent_dids['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_dids['month_year'].unique()\n",
    "dark_dids['month_year'].unique()\n",
    "consistent_dids['month_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend='plotly'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=len(lost_dids[\"month_year\"].unique()), \n",
    "    cols=2,\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]] * len(lost_dids[\"month_year\"].unique()),\n",
    "    row_titles=[f'{month}/{year}' for month, year in lost_dids['month_year'].unique()],\n",
    "    column_titles=['Count', 'Size']\n",
    ")\n",
    "monthly_stats_data = []\n",
    "\n",
    "for index, month_year in enumerate(lost_dids['month_year'].unique()):\n",
    "    month, year = month_year\n",
    "    monthly_lost_dids = lost_dids[(lost_dids['month'] == month) & (lost_dids['year'] == year)]\n",
    "    monthly_dark_dids = dark_dids[(dark_dids['month'] == month) & (dark_dids['year'] == year)]\n",
    "    monthly_consistent_dids = consistent_dids[(consistent_dids['month'] == month) & (consistent_dids['year'] == year)]\n",
    "\n",
    "    monthly_lost_dids_count = monthly_lost_dids['name'].nunique()\n",
    "    monthly_dark_dids_count = monthly_dark_dids['name'].nunique()\n",
    "    monthly_consistent_dids_count = monthly_consistent_dids['name'].nunique()\n",
    "    monthly_total_count = monthly_lost_dids_count + monthly_dark_dids_count + monthly_consistent_dids_count\n",
    "\n",
    "    monthly_lost_dids_size = monthly_lost_dids['size'].sum()\n",
    "    monthly_dark_dids_size = monthly_dark_dids['size'].sum()\n",
    "    monthly_consistent_dids_size = monthly_consistent_dids['size'].sum()\n",
    "    monthly_total_size = monthly_lost_dids_size + monthly_dark_dids_size + monthly_consistent_dids_size\n",
    "\n",
    "    monthly_df = pd.DataFrame(data={'Category': ['Lost', 'Dark', 'Consistent'], 'Count': [monthly_lost_dids_count, monthly_dark_dids_count, monthly_consistent_dids_count]})\n",
    "\n",
    "    monthly_stats = [\n",
    "        f\"{month}/{year}\",\n",
    "        monthly_lost_dids_count,\n",
    "        monthly_dark_dids_count,\n",
    "        monthly_consistent_dids_count,\n",
    "        monthly_total_count,\n",
    "        monthly_lost_dids_size,\n",
    "        monthly_dark_dids_size,\n",
    "        monthly_consistent_dids_size,\n",
    "        monthly_total_size\n",
    "    ]\n",
    "    monthly_stats_data.append(monthly_stats)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            textinfo='label+percent',\n",
    "            titleposition=\"bottom center\",\n",
    "            labels=['Lost', 'Dark', 'Consistent'],\n",
    "            values=[monthly_lost_dids_count, monthly_dark_dids_count, monthly_consistent_dids_count],\n",
    "            title=f'Count: {monthly_total_count} DIDs',\n",
    "            marker=dict(colors=['red', 'black', 'green'])\n",
    "        ), \n",
    "    row=index+1,\n",
    "    col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            textinfo=\"label+percent\",\n",
    "            titleposition=\"bottom center\",\n",
    "            labels=['Lost', 'Dark', 'Consistent'],\n",
    "            values=[monthly_lost_dids_size, monthly_dark_dids_size, monthly_consistent_dids_size],\n",
    "            title=f'Size: {bytesToTB(monthly_total_size)} TB',\n",
    "            marker=dict(colors=['red', 'black', 'green'])\n",
    "        ), \n",
    "        row=index+1,\n",
    "        col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=3000, width=1000, title_text=f\"SEAL RSE: Monthly Consistency Report: {time_range_start} to {time_range_end}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results of the monthly analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(f\"reports/{start_date}-{end_date}_monthly_consistency_report.html\")\n",
    "fig.write_image(f\"reports/{start_date}-{end_date}_monthly_consistency_report.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a line plot of the number of lost, dark and consistent DIDs over time using data frames in monthly_dfs\n",
    "monthly_stats_df = pd.DataFrame(columns=['Month/Year', 'Lost Count', 'Dark Count', 'Consistent Count', 'Total Count', 'Lost Size', 'Dark Size', 'Consistent Size', 'Total Size'], data=monthly_stats_data)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    subplot_titles=('Lost (number of DIDs)', 'Dark (number of DIDs)', 'Consistent (number of DIDs)')\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Lost Count'], mode='lines+markers', name='Lost DIDs'),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Dark Count'], mode='lines+markers', name='Dark DIDs'),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Consistent Count'], mode='lines+markers', name='Consistent DIDs'),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1000, width=1000, title_text=f\"SEAL RSE: Monthly Consistency Report: {time_range_start} to {time_range_end}\")\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(f\"reports/{start_date}-{end_date}_monthly_stats_by_number_of_dids.html\")\n",
    "fig.write_image(f\"reports/{start_date}-{end_date}_monthly_stats_by_number_of_dids.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    subplot_titles=('Lost (size of DIDs)', 'Dark (size of DIDs)', 'Consistent (size of DIDs)')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Lost Size'], mode='lines+markers', name='Lost DIDs'),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Dark Size'], mode='lines+markers', name='Dark DIDs'),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_stats_df['Month/Year'], y=monthly_stats_df['Consistent Size'], mode='lines+markers', name='Consistent DIDs'),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1000, width=1000, title_text=f\"SEAL RSE: Monthly Consistency Report: {time_range_start} to {time_range_end}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the monthly stats data frame\n",
    "\n",
    "monthly_stats_df.to_csv(f'data/outputs/{start_date}-{end_date}_monthly_stats.csv', index=False)\n",
    "\n",
    "\n",
    "# save the plots\n",
    "\n",
    "fig.write_html(f\"reports/{start_date}-{end_date}_monthly_stats_by_volume_report.html\")\n",
    "fig.write_image(f\"reports/{start_date}-{end_date}_monthly_stats_by_volume_report.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
